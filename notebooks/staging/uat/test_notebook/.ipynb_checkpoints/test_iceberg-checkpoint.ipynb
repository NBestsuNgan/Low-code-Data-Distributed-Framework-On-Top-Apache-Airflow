{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0aa3f-6e2e-4959-8338-aa8a85b9823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "REPAIR TABLE climate.weather;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3275c7a-6c62-4993-bff6-ad78c341c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE DATABASE IF NOT EXISTS climate;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a58cd-ec6b-487d-a412-e0e08f13dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE IF NOT EXISTS climate.weather (\n",
    "    datetime              timestamp,\n",
    "    temp                  double,\n",
    "    lat                   double,\n",
    "    long                  double,\n",
    "    cloud_coverage        string,\n",
    "    precip                double,\n",
    "    wind_speed            double\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (days(datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ce34e-8fd3-4efa-be19-a15abca9e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from climate.weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a026bd9-4eb3-4a54-934c-1758f5759c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #insert data\n",
    "# from datetime import datetime\n",
    "\n",
    "# schema = spark.table(\"climate.weather\").schema\n",
    "\n",
    "# data = [\n",
    "#     (datetime(2023,8,16), 76.2, 40.951908, -74.075272, \"Partially sunny\", 0.0, 3.5),\n",
    "#     (datetime(2023,8,17), 82.5, 40.951908, -74.075272, \"Sunny\", 0.0, 1.2),\n",
    "#     (datetime(2023,8,18), 70.9, 40.951908, -74.075272, \"Cloudy\", .5, 5.2)\n",
    "#   ]\n",
    "\n",
    "# df = spark.createDataFrame(data, schema)\n",
    "# df.writeTo(\"climate.weather\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50f932-bdc9-411e-b3b3-094d3390aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',  # Replace with your MinIO server URL\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "\n",
    "# # Initialize the Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Read Parquet from S3\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Bucket name and folder prefix (path)\n",
    "bucket_name = 'warehouse'\n",
    "folder_prefix = 'climate/weather/data/'\n",
    "\n",
    "# List all the objects in the 'data' folder\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix)\n",
    "\n",
    "# List to store Spark DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through all the objects and read each Parquet file\n",
    "for obj in response.get('Contents', []):\n",
    "    object_key = obj['Key']\n",
    "    \n",
    "    # Check if the object is a Parquet file (or if you want to apply other conditions)\n",
    "    if object_key.endswith('.parquet'):\n",
    "        # Get the Parquet file from MinIO\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "        parquet_data = response['Body'].read()\n",
    "        \n",
    "        # Read the Parquet data using PyArrow\n",
    "        table = pq.read_table(BytesIO(parquet_data))\n",
    "        \n",
    "        # Convert PyArrow Table to Spark DataFrame\n",
    "        df_spark = spark.createDataFrame(table.to_pandas())  # Convert pandas DataFrame to Spark DataFrame\n",
    "        \n",
    "        # Append the Spark DataFrame to the list\n",
    "        df_list.append(df_spark)\n",
    "\n",
    "# Combine all DataFrames into one Spark DataFrame if needed\n",
    "full_df_spark = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    full_df_spark = full_df_spark.union(df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "full_df_spark.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
